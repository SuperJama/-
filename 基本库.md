# urllib库
## request模块
### urlopen() 构造HTTP请求
```
urlopen(url,data,timeout,cafile,capath,cadefault,context)
```
该函数主要包含 read （） 、 readinto （）、 getheader(name ）、
getheaders （） 、 fileno （）等方法，以及 msg 、 version 、 status 、 reason 、 debuglevel 、 ιlosed 等属性<br>
timeout：超时时间<br>
### Request() 构建HTTP请求（功能强大）
```
Request (ur1, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
```
data：必须是字节流<br>
headers：请求头<br>
origin_req_host：请求方host/IP<br>
method：请求方法<br>

### 高级用法
HITPDefaultErrorHandler ：用于处理 HTTP 响应错误，错误都会抛出 HTTP Error 类型的异常 。<br>
HTTPRedirectHandler ：用于处理重定向 。<br>
HTTPCookieProcessor ： 用于处理 Cooki es 。<br>
ProxyHandler ：用于设置代理 ， 默认代理为空 。<br>
HπPPasswordMgr ：用于管理密码，它维护了用户名和密码的表 。<br>
HTTPBasicAuthHandler ： 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。<br>
```
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
p = HTTPPasswordMgrWithDefaultRealm()  #实例化
p.add_password()
q = HTTPBasicAuthHandler(p)
opener = build_opener(q)
opener.open(url)
```
### 代理
```
from urllib.request import ProxyHandler, build opener
proxy_handler = ProxyHandler({
    'http': 'http://127.0.0.1:9743',
    'https': 'https://127.0.0.1:9743'
 })
opener = build_opener(proxy_handler)
opener.open(url)
```
### 获取Cookies
```
import http .cookiejar, urllib.request
cookie = http. cookiejar. CookieJar()
handler = urllib .request.HTTPCookieProcessor (cookie)
opener = urllib.request . build opener(handler)
opener.open(url)
# 保存cookie
filename = ''
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open(url)
cookie.save(ignore_discard=True, ignore_expires=True)
# 使用保存的cookie
cookie = http.cookiejar.LWPCookieJar()
cookie.load(filename, ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib .request.build_opener(handler)
response= opener.open(url)
```
## 解析链接
### urlparse()
```
from urllib.parse import urlparse
urlparse(url, sheme='',allow_fragments=True)
```
sheme：默认协议
allow_fragment：是否忽略fragment
会将url分成6部分<br>
标准链接格式：<br>
http://www.baidu.com/index.html;user?id=S#comment <br>
scheme://netloc/path ;params?query#fragment <br>
### urlunparse() 和urlparse相反
```
from urllib.parse import urlunparse
data =['http','www.baidu.com','index.html','user','a=6','comment']
urlunparse(data)
```
### urlsplit() 和urlprase相似
不单独解析params，返回5部分<br>

### urlunsplit()同上

### urljoin() 合并补充链接
```
from urllib.parse import urljoin
urljoin(base_url, new_url)
```
该函数会将base_url有而new_url没有的补充在new_url上<br>

### urlencode() 构造GET请求参数
```
from urllib .parse import urlencode
params = {
  'name':'germey',
  'age' : 22
 }
 base_url = ''
 url = base_url + urlencode(params)
```
序列化：将字典params序列化为参数<br>

### parse_qs() 反序列化
```
from urllib.parse import parse_qs
query = 'name=germey&age=22'
parse_qs(query)
```
将query反序列化为字典<br>

### parse_qsl() 反序列化为元组组成的列表

### quote() 将内容转化为URL编码
```
from urllib.parse import quote
keyword ＝ '壁纸'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
```
'壁纸'被转码为'%E5%A3%81%E7%BA%B8'<br>

### unquote() URL解码

## 分析Robots协议
### Robots协议示例：
```
User-agent: *
Disallow: I
Allow: /public/
```
## robotparser模块
```
urllib.robotparser.RobotFileParser(url)
```
下面是几个引用函数：<br>
set_url(): 用来引入url <br>
read(): 读取robots文件并分析 <br>
parse(): 分析robots内容 <br>
can_fetch(): 传入User-agent和URL，查看是否能够抓取 <br>
mtime(): 返回上次抓取分析robots的时间 <br>
modified(): 将当前时间设置为上次抓取和分析robots的时间 <br>












